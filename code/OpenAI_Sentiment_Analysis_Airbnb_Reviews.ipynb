{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Sentiment Analysis for Airbnb Reviews\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates sentiment analysis of Airbnb review comments using OpenAI's Large Language Models (LLMs). The solution uses prompt engineering techniques to classify reviews as positive (1), neutral (0), or negative (-1).\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Dependencies](#setup)\n",
    "2. [Configuration](#configuration)\n",
    "3. [Data Loading](#data-loading)\n",
    "4. [Prompt Engineering](#prompt-engineering)\n",
    "5. [Sentiment Prediction](#sentiment-prediction)\n",
    "6. [Results Analysis](#results-analysis)\n",
    "7. [Cost Tracking](#cost-tracking)\n",
    "8. [Export Results](#export-results)\n",
    "9. [Summary](#summary)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- OpenAI API key\n",
    "- Python 3.11+\n",
    "- Required packages: openai, pandas, numpy, matplotlib, python-dotenv\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Flexible Model Selection**: Support for GPT-4, GPT-3.5-turbo, and other OpenAI models\n",
    "- **Robust Error Handling**: Automatic retry with exponential backoff for rate limits\n",
    "- **Cost Tracking**: Monitor token usage and estimate API costs\n",
    "- **Batch Processing**: Efficiently process multiple reviews with progress tracking\n",
    "- **Results Export**: Save predictions to CSV for downstream analysis\n",
    "- **Visualization**: Charts and metrics for sentiment distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Dependencies <a id='setup'></a>\n",
    "\n",
    "Install required packages and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "# Uncomment the following line to install packages\n",
    "# !pip install openai pandas numpy matplotlib python-dotenv hypothesis pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "import json\n",
    "import getpass\n",
    "\n",
    "# Import data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Import OpenAI\n",
    "from openai import OpenAI\n",
    "from openai import OpenAIError, RateLimitError, APIError, APIConnectionError, Timeout\n",
    "\n",
    "# Import environment variable management\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Logging\n",
    "\n",
    "Set up logging to track API calls, errors, and processing progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('openai_sentiment_analysis.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"OpenAI Sentiment Analysis Notebook initialized\")\n",
    "logger.info(f\"Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n",
    "\n",
    "Load API keys and configuration from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "logger.info(\"Environment variables loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Installation\n",
    "\n",
    "Check that all required packages are installed and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify package versions\n",
    "import openai\n",
    "\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  OpenAI: {openai.__version__}\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  Python: {sys.version}\")\n",
    "\n",
    "logger.info(\"All required packages verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration <a id='configuration'></a>\n",
    "\n",
    "Configure the OpenAI API client and set analysis parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Dataclass\n",
    "\n",
    "Define a configuration dataclass to manage all parameters for the sentiment analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for OpenAI sentiment analysis.\n",
    "    \n",
    "    This dataclass centralizes all configuration parameters including API credentials,\n",
    "    model settings, processing options, and file paths.\n",
    "    \n",
    "    Attributes:\n",
    "        api_key: OpenAI API key for authentication\n",
    "        model_name: Name of the OpenAI model to use (e.g., 'gpt-3.5-turbo', 'gpt-4')\n",
    "        temperature: Sampling temperature (0.0 for deterministic, higher for creative)\n",
    "        max_tokens: Maximum tokens in model response\n",
    "        batch_size: Number of reviews to process in each batch\n",
    "        rate_limit_delay: Delay in seconds between API requests\n",
    "        max_reviews: Maximum number of reviews to process (None for all)\n",
    "        data_file: Path to input CSV file with reviews\n",
    "        output_dir: Directory for saving results\n",
    "        max_retries: Maximum number of retry attempts for failed requests\n",
    "        backoff_factor: Multiplier for exponential backoff delays\n",
    "    \"\"\"\n",
    "    \n",
    "    # API Configuration\n",
    "    api_key: str\n",
    "    model_name: str = \"gpt-3.5-turbo\"\n",
    "    \n",
    "    # Model Parameters\n",
    "    temperature: float = 0.0  # Deterministic for consistency\n",
    "    max_tokens: int = 10      # Short responses for sentiment labels\n",
    "    \n",
    "    # Processing Configuration\n",
    "    batch_size: int = 100\n",
    "    max_reviews: Optional[int] = None\n",
    "    rate_limit_delay: float = 0.5  # Seconds between requests\n",
    "    \n",
    "    # File Configuration\n",
    "    data_file: str = \"reviews/paris-2015-09-02-reviews.csv\"\n",
    "    output_dir: str = \"results/\"\n",
    "    \n",
    "    # Retry Configuration\n",
    "    max_retries: int = 3\n",
    "    backoff_factor: float = 2.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration parameters after initialization.\"\"\"\n",
    "        self._validate()\n",
    "    \n",
    "    def _validate(self) -> None:\n",
    "        \"\"\"Validate configuration parameters.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If any parameter is invalid\n",
    "        \"\"\"\n",
    "        # Validate API key\n",
    "        if not self.api_key or not isinstance(self.api_key, str):\n",
    "            raise ValueError(\"API key must be a non-empty string\")\n",
    "        \n",
    "        if len(self.api_key.strip()) == 0:\n",
    "            raise ValueError(\"API key cannot be empty or whitespace\")\n",
    "        \n",
    "        # Validate model name\n",
    "        if not self.model_name or not isinstance(self.model_name, str):\n",
    "            raise ValueError(\"Model name must be a non-empty string\")\n",
    "        \n",
    "        # Validate temperature\n",
    "        if not isinstance(self.temperature, (int, float)):\n",
    "            raise ValueError(\"Temperature must be a number\")\n",
    "        \n",
    "        if not 0.0 <= self.temperature <= 2.0:\n",
    "            raise ValueError(\"Temperature must be between 0.0 and 2.0\")\n",
    "        \n",
    "        # Validate max_tokens\n",
    "        if not isinstance(self.max_tokens, int):\n",
    "            raise ValueError(\"max_tokens must be an integer\")\n",
    "        \n",
    "        if self.max_tokens <= 0:\n",
    "            raise ValueError(\"max_tokens must be positive\")\n",
    "        \n",
    "        # Validate batch_size\n",
    "        if not isinstance(self.batch_size, int):\n",
    "            raise ValueError(\"batch_size must be an integer\")\n",
    "        \n",
    "        if self.batch_size <= 0:\n",
    "            raise ValueError(\"batch_size must be positive\")\n",
    "        \n",
    "        # Validate rate_limit_delay\n",
    "        if not isinstance(self.rate_limit_delay, (int, float)):\n",
    "            raise ValueError(\"rate_limit_delay must be a number\")\n",
    "        \n",
    "        if self.rate_limit_delay < 0:\n",
    "            raise ValueError(\"rate_limit_delay must be non-negative\")\n",
    "        \n",
    "        # Validate max_reviews\n",
    "        if self.max_reviews is not None:\n",
    "            if not isinstance(self.max_reviews, int):\n",
    "                raise ValueError(\"max_reviews must be an integer or None\")\n",
    "            \n",
    "            if self.max_reviews <= 0:\n",
    "                raise ValueError(\"max_reviews must be positive\")\n",
    "        \n",
    "        # Validate data_file\n",
    "        if not self.data_file or not isinstance(self.data_file, str):\n",
    "            raise ValueError(\"data_file must be a non-empty string\")\n",
    "        \n",
    "        # Validate output_dir\n",
    "        if not self.output_dir or not isinstance(self.output_dir, str):\n",
    "            raise ValueError(\"output_dir must be a non-empty string\")\n",
    "        \n",
    "        # Validate max_retries\n",
    "        if not isinstance(self.max_retries, int):\n",
    "            raise ValueError(\"max_retries must be an integer\")\n",
    "        \n",
    "        if self.max_retries < 0:\n",
    "            raise ValueError(\"max_retries must be non-negative\")\n",
    "        \n",
    "        # Validate backoff_factor\n",
    "        if not isinstance(self.backoff_factor, (int, float)):\n",
    "            raise ValueError(\"backoff_factor must be a number\")\n",
    "        \n",
    "        if self.backoff_factor < 1.0:\n",
    "            raise ValueError(\"backoff_factor must be >= 1.0\")\n",
    "        \n",
    "        logger.info(\"Configuration validation passed\")\n",
    "    \n",
    "    def display(self) -> None:\n",
    "        \"\"\"Display configuration with sensitive data masked.\n",
    "        \n",
    "        Prints all configuration parameters in a readable format,\n",
    "        masking the API key for security.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Configuration Settings\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(\"\\nAPI Configuration:\")\n",
    "        print(f\"  API Key: {self._mask_api_key(self.api_key)}\")\n",
    "        print(f\"  Model Name: {self.model_name}\")\n",
    "        \n",
    "        print(\"\\nModel Parameters:\")\n",
    "        print(f\"  Temperature: {self.temperature}\")\n",
    "        print(f\"  Max Tokens: {self.max_tokens}\")\n",
    "        \n",
    "        print(\"\\nProcessing Configuration:\")\n",
    "        print(f\"  Batch Size: {self.batch_size}\")\n",
    "        print(f\"  Max Reviews: {self.max_reviews if self.max_reviews else 'All'}\")\n",
    "        print(f\"  Rate Limit Delay: {self.rate_limit_delay}s\")\n",
    "        \n",
    "        print(\"\\nFile Configuration:\")\n",
    "        print(f\"  Data File: {self.data_file}\")\n",
    "        print(f\"  Output Directory: {self.output_dir}\")\n",
    "        \n",
    "        print(\"\\nRetry Configuration:\")\n",
    "        print(f\"  Max Retries: {self.max_retries}\")\n",
    "        print(f\"  Backoff Factor: {self.backoff_factor}\")\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _mask_api_key(api_key: str) -> str:\n",
    "        \"\"\"Mask API key for secure display.\n",
    "        \n",
    "        Args:\n",
    "            api_key: The API key to mask\n",
    "        \n",
    "        Returns:\n",
    "            Masked API key showing only first 4 and last 4 characters\n",
    "        \"\"\"\n",
    "        if len(api_key) <= 8:\n",
    "            return \"*\" * len(api_key)\n",
    "        \n",
    "        return f\"{api_key[:4]}...{api_key[-4:]}\"\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert configuration to dictionary.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary representation of configuration (with masked API key)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"api_key\": self._mask_api_key(self.api_key),\n",
    "            \"model_name\": self.model_name,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"max_reviews\": self.max_reviews,\n",
    "            \"rate_limit_delay\": self.rate_limit_delay,\n",
    "            \"data_file\": self.data_file,\n",
    "            \"output_dir\": self.output_dir,\n",
    "            \"max_retries\": self.max_retries,\n",
    "            \"backoff_factor\": self.backoff_factor\n",
    "        }\n",
    "\n",
    "logger.info(\"Config dataclass defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secure API Key Input\n",
    "\n",
    "Securely obtain the OpenAI API key from environment variables or user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_key() -> str:\n",
    "    \"\"\"Securely obtain OpenAI API key.\n",
    "    \n",
    "    Attempts to load API key from environment variable first.\n",
    "    If not found, prompts user for secure input.\n",
    "    \n",
    "    Returns:\n",
    "        OpenAI API key\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If API key is not provided or is empty\n",
    "    \"\"\"\n",
    "    # Try to get from environment variable\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    if api_key and len(api_key.strip()) > 0:\n",
    "        logger.info(\"API key loaded from environment variable\")\n",
    "        return api_key.strip()\n",
    "    \n",
    "    # If not in environment, prompt user\n",
    "    logger.info(\"API key not found in environment, prompting user\")\n",
    "    print(\"\\nOpenAI API key not found in environment variables.\")\n",
    "    print(\"Please enter your OpenAI API key (input will be hidden):\")\n",
    "    \n",
    "    api_key = getpass.getpass(\"API Key: \")\n",
    "    \n",
    "    if not api_key or len(api_key.strip()) == 0:\n",
    "        raise ValueError(\"API key cannot be empty\")\n",
    "    \n",
    "    logger.info(\"API key obtained from user input\")\n",
    "    return api_key.strip()\n",
    "\n",
    "logger.info(\"API key input function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Configuration\n",
    "\n",
    "Create a configuration instance with default or custom parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key securely\n",
    "api_key = get_api_key()\n",
    "\n",
    "# Create configuration with default settings\n",
    "config = Config(\n",
    "    api_key=api_key,\n",
    "    model_name=\"gpt-3.5-turbo\",  # Use GPT-3.5-turbo for cost efficiency\n",
    "    temperature=0.0,              # Deterministic responses for consistency\n",
    "    max_tokens=10,                # Short responses (just sentiment label)\n",
    "    batch_size=100,               # Process 100 reviews at a time\n",
    "    rate_limit_delay=0.5,         # 0.5 second delay between requests\n",
    "    max_reviews=None,             # Process all reviews (set to number for testing)\n",
    "    data_file=\"paris-2015-09-02-reviews.csv\",  # Default to Paris dataset\n",
    "    output_dir=\"results/\",        # Save results to results directory\n",
    "    max_retries=3,                # Retry failed requests up to 3 times\n",
    "    backoff_factor=2.0            # Double delay on each retry\n",
    ")\n",
    "\n",
    "# Display configuration (with masked API key)\n",
    "config.display()\n",
    "\n",
    "logger.info(\"Configuration initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Notes\n",
    "\n",
    "**Model Selection:**\n",
    "- `gpt-3.5-turbo`: Fast and cost-effective for sentiment analysis\n",
    "- `gpt-4`: More accurate but significantly more expensive\n",
    "\n",
    "**Temperature:**\n",
    "- Set to 0.0 for deterministic, consistent sentiment predictions\n",
    "- Higher values (0.5-1.0) introduce randomness\n",
    "\n",
    "**Rate Limiting:**\n",
    "- Adjust `rate_limit_delay` based on your API tier\n",
    "- Free tier: 3 requests/minute → use 20+ second delay\n",
    "- Paid tier: Higher limits → can reduce delay\n",
    "\n",
    "**Testing:**\n",
    "- Set `max_reviews` to a small number (e.g., 50) for initial testing\n",
    "- Set to `None` to process entire dataset\n",
    "\n",
    "**Cost Optimization:**\n",
    "- Use `gpt-3.5-turbo` instead of `gpt-4` (10-20x cheaper)\n",
    "- Keep `max_tokens` low (sentiment labels are short)\n",
    "- Sample large datasets instead of processing all reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading <a id='data-loading'></a>\n",
    "\n",
    "Load and preprocess Airbnb review data from CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReviewDataLoader Class\n",
    "\n",
    "Create a class to handle loading and preprocessing of review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataLoader:\n",
    "    \"\"\"Loads and preprocesses Airbnb review data from CSV files.\n",
    "    \n",
    "    This class handles:\n",
    "    - Loading CSV files with error handling\n",
    "    - Extracting and filtering comment columns\n",
    "    - Validating data structure\n",
    "    - Providing sample records for inspection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the ReviewDataLoader.\"\"\"\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.info(\"ReviewDataLoader initialized\")\n",
    "    \n",
    "    def load_reviews(self, file_path: str, max_rows: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"Load reviews from CSV file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to CSV file containing reviews\n",
    "            max_rows: Maximum number of rows to load (None for all)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame containing review data\n",
    "        \n",
    "        Raises:\n",
    "            FileNotFoundError: If file does not exist\n",
    "            ValueError: If file is empty or invalid\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Loading reviews from: {file_path}\")\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            error_msg = f\"File not found: {file_path}\"\n",
    "            self.logger.error(error_msg)\n",
    "            raise FileNotFoundError(error_msg)\n",
    "        \n",
    "        try:\n",
    "            # Load CSV file\n",
    "            if max_rows is not None:\n",
    "                df = pd.read_csv(file_path, nrows=max_rows)\n",
    "                self.logger.info(f\"Loaded {len(df)} rows (limited to {max_rows})\")\n",
    "            else:\n",
    "                df = pd.read_csv(file_path)\n",
    "                self.logger.info(f\"Loaded {len(df)} rows\")\n",
    "            \n",
    "            # Check if DataFrame is empty\n",
    "            if df.empty:\n",
    "                error_msg = f\"File is empty: {file_path}\"\n",
    "                self.logger.error(error_msg)\n",
    "                raise ValueError(error_msg)\n",
    "            \n",
    "            # Validate data structure\n",
    "            if not self.validate_data(df):\n",
    "                error_msg = f\"Invalid data structure in file: {file_path}\"\n",
    "                self.logger.error(error_msg)\n",
    "                raise ValueError(error_msg)\n",
    "            \n",
    "            self.logger.info(f\"Successfully loaded {len(df)} reviews\")\n",
    "            self.logger.info(f\"Columns: {list(df.columns)}\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except pd.errors.EmptyDataError:\n",
    "            error_msg = f\"File is empty or invalid: {file_path}\"\n",
    "            self.logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        except pd.errors.ParserError as e:\n",
    "            error_msg = f\"Error parsing CSV file: {e}\"\n",
    "            self.logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Unexpected error loading file: {e}\"\n",
    "            self.logger.error(error_msg)\n",
    "            raise\n",
    "    \n",
    "    def extract_comments(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Extract comment column and filter out missing values.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing review data\n",
    "        \n",
    "        Returns:\n",
    "            List of non-null comment strings\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If 'comments' column does not exist\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Extracting comments from DataFrame\")\n",
    "        \n",
    "        # Check if comments column exists\n",
    "        if 'comments' not in df.columns:\n",
    "            error_msg = \"'comments' column not found in DataFrame\"\n",
    "            self.logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        # Get total count before filtering\n",
    "        total_count = len(df)\n",
    "        \n",
    "        # Extract comments and filter null/empty values\n",
    "        comments = df['comments'].dropna()\n",
    "        \n",
    "        # Convert to string and filter empty strings\n",
    "        comments = comments.astype(str)\n",
    "        comments = comments[comments.str.strip() != '']\n",
    "        \n",
    "        # Convert to list\n",
    "        comment_list = comments.tolist()\n",
    "        \n",
    "        filtered_count = total_count - len(comment_list)\n",
    "        self.logger.info(f\"Extracted {len(comment_list)} comments\")\n",
    "        self.logger.info(f\"Filtered out {filtered_count} null/empty comments\")\n",
    "        \n",
    "        return comment_list\n",
    "    \n",
    "    def validate_data(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"Validate that required columns exist in DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to validate\n",
    "        \n",
    "        Returns:\n",
    "            True if data is valid, False otherwise\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Validating DataFrame structure\")\n",
    "        \n",
    "        # Check if 'comments' column exists\n",
    "        if 'comments' not in df.columns:\n",
    "            self.logger.warning(\"'comments' column not found\")\n",
    "            return False\n",
    "        \n",
    "        self.logger.info(\"DataFrame validation passed\")\n",
    "        return True\n",
    "    \n",
    "    def get_sample(self, df: pd.DataFrame, n: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Get sample records for display.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to sample from\n",
    "            n: Number of samples to return\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with n sample records\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Getting {n} sample records\")\n",
    "        \n",
    "        # Return min of n or total rows\n",
    "        sample_size = min(n, len(df))\n",
    "        sample = df.head(sample_size)\n",
    "        \n",
    "        self.logger.info(f\"Returning {len(sample)} sample records\")\n",
    "        return sample\n",
    "\n",
    "logger.info(\"ReviewDataLoader class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Review Data\n",
    "\n",
    "Load the review dataset and display sample records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = ReviewDataLoader()\n",
    "\n",
    "# Load reviews from configured file\n",
    "reviews_df = data_loader.load_reviews(config.data_file, config.max_reviews)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset Statistics\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total reviews loaded: {len(reviews_df)}\")\n",
    "print(f\"Columns: {list(reviews_df.columns)}\")\n",
    "print(f\"Memory usage: {reviews_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "logger.info(f\"Loaded {len(reviews_df)} reviews from {config.data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Sample Reviews\n",
    "\n",
    "Show a few sample reviews to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample records\n",
    "sample_df = data_loader.get_sample(reviews_df, n=5)\n",
    "\n",
    "print(\"\\nSample Reviews:\")\n",
    "print(\"=\"*60)\n",
    "display(sample_df[['comments']].head())\n",
    "\n",
    "logger.info(\"Sample reviews displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Comments\n",
    "\n",
    "Extract and filter comment text for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract comments\n",
    "comments = data_loader.extract_comments(reviews_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comment Extraction Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total comments extracted: {len(comments)}\")\n",
    "print(f\"Average comment length: {np.mean([len(c) for c in comments]):.1f} characters\")\n",
    "print(f\"Shortest comment: {min([len(c) for c in comments])} characters\")\n",
    "print(f\"Longest comment: {max([len(c) for c in comments])} characters\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Display a few sample comments\n",
    "print(\"Sample Comments:\")\n",
    "for i, comment in enumerate(comments[:3], 1):\n",
    "    print(f\"\\n{i}. {comment[:200]}...\" if len(comment) > 200 else f\"\\n{i}. {comment}\")\n",
    "\n",
    "logger.info(f\"Extracted {len(comments)} comments for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Prompt Engineering <a id='prompt-engineering'></a>\n",
    "\n",
    "Design and implement prompts for sentiment classification using OpenAI LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptEngine Class\n",
    "\n",
    "Create a class to manage prompt construction and response validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptEngine:\n",
    "    \"\"\"Manages prompt construction for sentiment analysis.\n",
    "    \n",
    "    This class handles:\n",
    "    - System message definition\n",
    "    - Few-shot example management\n",
    "    - Prompt construction\n",
    "    - Response validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, few_shot_examples: Optional[List[Dict]] = None):\n",
    "        \"\"\"Initialize the PromptEngine.\n",
    "        \n",
    "        Args:\n",
    "            few_shot_examples: Optional list of example dictionaries with 'comment' and 'sentiment' keys\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "        # Use default examples if none provided\n",
    "        if few_shot_examples is None:\n",
    "            self.few_shot_examples = self._get_default_examples()\n",
    "        else:\n",
    "            self.few_shot_examples = few_shot_examples\n",
    "        \n",
    "        self.logger.info(f\"PromptEngine initialized with {len(self.few_shot_examples)} few-shot examples\")\n",
    "    \n",
    "    def _get_default_examples(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get default few-shot examples.\n",
    "        \n",
    "        Returns:\n",
    "            List of example dictionaries\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"comment\": \"The apartment was amazing! Great location and very clean. The host was super responsive and helpful. Would definitely stay here again!\",\n",
    "                \"sentiment\": 1\n",
    "            },\n",
    "            {\n",
    "                \"comment\": \"The place was okay, nothing special. It served its purpose for a short stay.\",\n",
    "                \"sentiment\": 0\n",
    "            },\n",
    "            {\n",
    "                \"comment\": \"Terrible experience. The apartment was dirty and the host was unresponsive. The photos were misleading. Would not recommend.\",\n",
    "                \"sentiment\": -1\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def get_system_message(self) -> str:\n",
    "        \"\"\"Get system message defining the task.\n",
    "        \n",
    "        Returns:\n",
    "            System message string\n",
    "        \"\"\"\n",
    "        return (\n",
    "            \"You are a sentiment analysis expert. Your task is to analyze Airbnb review comments \"\n",
    "            \"and classify them into one of three sentiment categories:\\n\\n\"\n",
    "            \"- Positive (1): The review expresses satisfaction, praise, or positive experiences\\n\"\n",
    "            \"- Neutral (0): The review is balanced, factual, or neither clearly positive nor negative\\n\"\n",
    "            \"- Negative (-1): The review expresses dissatisfaction, complaints, or negative experiences\\n\\n\"\n",
    "            \"Respond with ONLY the sentiment label: 1, 0, or -1. Do not include any explanation or additional text.\"\n",
    "        )\n",
    "    \n",
    "    def get_few_shot_examples(self) -> str:\n",
    "        \"\"\"Format few-shot examples for the prompt.\n",
    "        \n",
    "        Returns:\n",
    "            Formatted few-shot examples string\n",
    "        \"\"\"\n",
    "        examples_text = \"Here are some examples:\\n\\n\"\n",
    "        \n",
    "        for i, example in enumerate(self.few_shot_examples, 1):\n",
    "            examples_text += f\"Example {i}:\\n\"\n",
    "            examples_text += f\"Comment: {example['comment']}\\n\"\n",
    "            examples_text += f\"Sentiment: {example['sentiment']}\\n\\n\"\n",
    "        \n",
    "        return examples_text\n",
    "    \n",
    "    def build_prompt(self, comment: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Construct complete prompt for a comment.\n",
    "        \n",
    "        Args:\n",
    "            comment: Review comment to analyze\n",
    "        \n",
    "        Returns:\n",
    "            List of message dictionaries for OpenAI API\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": self.get_system_message()\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": self.get_few_shot_examples() + f\"Now analyze this comment:\\n\\nComment: {comment}\\n\\nSentiment:\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return messages\n",
    "    \n",
    "    def validate_response(self, response: str) -> bool:\n",
    "        \"\"\"Validate that response is a valid sentiment label.\n",
    "        \n",
    "        Args:\n",
    "            response: Response string from LLM\n",
    "        \n",
    "        Returns:\n",
    "            True if response is valid (-1, 0, or 1), False otherwise\n",
    "        \"\"\"\n",
    "        # Strip whitespace and try to convert to int\n",
    "        try:\n",
    "            sentiment = int(response.strip())\n",
    "            return sentiment in [-1, 0, 1]\n",
    "        except (ValueError, AttributeError):\n",
    "            return False\n",
    "    \n",
    "    def parse_sentiment(self, response: str) -> int:\n",
    "        \"\"\"Parse sentiment from response string.\n",
    "        \n",
    "        Args:\n",
    "            response: Response string from LLM\n",
    "        \n",
    "        Returns:\n",
    "            Sentiment value (-1, 0, or 1)\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If response cannot be parsed as valid sentiment\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sentiment = int(response.strip())\n",
    "            if sentiment not in [-1, 0, 1]:\n",
    "                raise ValueError(f\"Invalid sentiment value: {sentiment}\")\n",
    "            return sentiment\n",
    "        except (ValueError, AttributeError) as e:\n",
    "            self.logger.warning(f\"Failed to parse sentiment from response: {response}\")\n",
    "            raise ValueError(f\"Invalid sentiment response: {response}\") from e\n",
    "\n",
    "logger.info(\"PromptEngine class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Prompt Engine\n",
    "\n",
    "Create a prompt engine instance with default few-shot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize prompt engine\n",
    "prompt_engine = PromptEngine()\n",
    "\n",
    "logger.info(\"Prompt engine initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Prompt Structure\n",
    "\n",
    "Show the system message and few-shot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Prompt Structure\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nSystem Message:\")\n",
    "print(\"-\" * 60)\n",
    "print(prompt_engine.get_system_message())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Few-Shot Examples:\")\n",
    "print(\"=\"*60)\n",
    "print(prompt_engine.get_few_shot_examples())\n",
    "\n",
    "logger.info(\"Prompt structure displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Prompt Construction\n",
    "\n",
    "Build a sample prompt to verify the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample comment\n",
    "test_comment = \"The location was perfect and the apartment was clean. However, the wifi was slow.\"\n",
    "\n",
    "# Build prompt\n",
    "test_messages = prompt_engine.build_prompt(test_comment)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample Prompt Construction\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest Comment: {test_comment}\")\n",
    "print(\"\\nConstructed Messages:\")\n",
    "for i, msg in enumerate(test_messages, 1):\n",
    "    print(f\"\\nMessage {i} ({msg['role']}):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(msg['content'][:300] + \"...\" if len(msg['content']) > 300 else msg['content'])\n",
    "\n",
    "logger.info(\"Prompt construction test completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Sentiment Prediction <a id='sentiment-prediction'></a>\n",
    "\n",
    "Implement OpenAI API integration for sentiment prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAIClient Class\n",
    "\n",
    "Create a wrapper class for OpenAI API with error handling and response parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIClient:\n",
    "    \"\"\"Wrapper for OpenAI API with error handling and rate limiting.\n",
    "    \n",
    "    This class handles:\n",
    "    - API authentication and initialization\n",
    "    - Single and batch sentiment predictions\n",
    "    - Response parsing and validation\n",
    "    - Error handling with retries\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str, config: Config):\n",
    "        \"\"\"Initialize OpenAI client.\n",
    "        \n",
    "        Args:\n",
    "            api_key: OpenAI API key\n",
    "            model: Model name (e.g., 'gpt-3.5-turbo')\n",
    "            config: Configuration object\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        \n",
    "        self.logger.info(f\"OpenAIClient initialized with model: {model}\")\n",
    "    \n",
    "    def predict_sentiment(self, comment: str, messages: List[Dict[str, str]]) -> Dict[str, Any]:\n",
    "        \"\"\"Predict sentiment for a single comment.\n",
    "        \n",
    "        Args:\n",
    "            comment: Review comment to analyze\n",
    "            messages: Formatted messages for API\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with sentiment, tokens, and metadata\n",
    "        \"\"\"\n",
    "        retry_count = 0\n",
    "        last_error = None\n",
    "        \n",
    "        while retry_count <= self.config.max_retries:\n",
    "            try:\n",
    "                # Make API call\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=messages,\n",
    "                    temperature=self.config.temperature,\n",
    "                    max_tokens=self.config.max_tokens\n",
    "                )\n",
    "                \n",
    "                # Parse response\n",
    "                sentiment, input_tokens, output_tokens = self._parse_response(response)\n",
    "                \n",
    "                return {\n",
    "                    \"comment\": comment,\n",
    "                    \"sentiment\": sentiment,\n",
    "                    \"input_tokens\": input_tokens,\n",
    "                    \"output_tokens\": output_tokens,\n",
    "                    \"error\": None\n",
    "                }\n",
    "                \n",
    "            except RateLimitError as e:\n",
    "                last_error = e\n",
    "                retry_count += 1\n",
    "                if retry_count <= self.config.max_retries:\n",
    "                    delay = self._calculate_backoff(retry_count)\n",
    "                    self.logger.warning(f\"Rate limit hit. Retry {retry_count}/{self.config.max_retries} after {delay}s\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    self.logger.error(f\"Max retries exceeded for rate limit: {e}\")\n",
    "                    \n",
    "            except (APIError, APIConnectionError, Timeout) as e:\n",
    "                last_error = e\n",
    "                retry_count += 1\n",
    "                if retry_count <= self.config.max_retries:\n",
    "                    delay = self._calculate_backoff(retry_count)\n",
    "                    self.logger.warning(f\"API error. Retry {retry_count}/{self.config.max_retries} after {delay}s: {e}\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    self.logger.error(f\"Max retries exceeded for API error: {e}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Unexpected error: {e}\")\n",
    "                last_error = e\n",
    "                break\n",
    "        \n",
    "        # Return error result if all retries failed\n",
    "        return {\n",
    "            \"comment\": comment,\n",
    "            \"sentiment\": 0,  # Default to neutral\n",
    "            \"input_tokens\": 0,\n",
    "            \"output_tokens\": 0,\n",
    "            \"error\": str(last_error)\n",
    "        }\n",
    "    \n",
    "    def _parse_response(self, response: Any) -> Tuple[int, int, int]:\n",
    "        \"\"\"Extract sentiment and token counts from API response.\n",
    "        \n",
    "        Args:\n",
    "            response: OpenAI API response object\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (sentiment, input_tokens, output_tokens)\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If response cannot be parsed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract response text\n",
    "            response_text = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Parse sentiment\n",
    "            sentiment = int(response_text)\n",
    "            \n",
    "            # Validate sentiment\n",
    "            if sentiment not in [-1, 0, 1]:\n",
    "                self.logger.warning(f\"Invalid sentiment value: {sentiment}, defaulting to 0\")\n",
    "                sentiment = 0\n",
    "            \n",
    "            # Extract token counts\n",
    "            input_tokens = response.usage.prompt_tokens\n",
    "            output_tokens = response.usage.completion_tokens\n",
    "            \n",
    "            return sentiment, input_tokens, output_tokens\n",
    "            \n",
    "        except (ValueError, AttributeError, IndexError) as e:\n",
    "            self.logger.warning(f\"Failed to parse response, defaulting to neutral: {e}\")\n",
    "            # Default to neutral sentiment if parsing fails\n",
    "            return 0, 0, 0\n",
    "    \n",
    "    def _calculate_backoff(self, retry_count: int) -> float:\n",
    "        \"\"\"Calculate exponential backoff delay.\n",
    "        \n",
    "        Args:\n",
    "            retry_count: Current retry attempt number\n",
    "        \n",
    "        Returns:\n",
    "            Delay in seconds\n",
    "        \"\"\"\n",
    "        base_delay = self.config.rate_limit_delay\n",
    "        delay = base_delay * (self.config.backoff_factor ** (retry_count - 1))\n",
    "        return delay\n",
    "\n",
    "logger.info(\"OpenAIClient class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OpenAI Client\n",
    "\n",
    "Create an OpenAI client instance with the configured API key and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "openai_client = OpenAIClient(\n",
    "    api_key=config.api_key,\n",
    "    model=config.model_name,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "logger.info(\"OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Single Prediction\n",
    "\n",
    "Test the sentiment prediction with a sample comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample comment\n",
    "test_comment = \"The apartment was clean and the host was friendly. Great experience overall!\"\n",
    "\n",
    "# Build prompt\n",
    "test_messages = prompt_engine.build_prompt(test_comment)\n",
    "\n",
    "# Predict sentiment\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Single Prediction\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nComment: {test_comment}\")\n",
    "print(\"\\nMaking API call...\")\n",
    "\n",
    "result = openai_client.predict_sentiment(test_comment, test_messages)\n",
    "\n",
    "print(\"\\nResult:\")\n",
    "print(f\"  Sentiment: {result['sentiment']}\")\n",
    "print(f\"  Input Tokens: {result['input_tokens']}\")\n",
    "print(f\"  Output Tokens: {result['output_tokens']}\")\n",
    "print(f\"  Error: {result['error']}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "logger.info(\"Single prediction test completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Prediction Function\n",
    "\n",
    "Implement batch processing with progress tracking and error resilience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(comments: List[str], client: OpenAIClient, prompt_engine: PromptEngine, \n",
    "                  rate_limit_delay: float = 0.5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Predict sentiment for multiple comments with progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        comments: List of review comments\n",
    "        client: OpenAI client instance\n",
    "        prompt_engine: Prompt engine instance\n",
    "        rate_limit_delay: Delay between requests in seconds\n",
    "    \n",
    "    Returns:\n",
    "        List of prediction result dictionaries\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total = len(comments)\n",
    "    \n",
    "    logger.info(f\"Starting batch prediction for {total} comments\")\n",
    "    print(f\"\\nProcessing {total} comments...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, comment in enumerate(comments, 1):\n",
    "        try:\n",
    "            # Build prompt\n",
    "            messages = prompt_engine.build_prompt(comment)\n",
    "            \n",
    "            # Predict sentiment\n",
    "            result = client.predict_sentiment(comment, messages)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if i % 10 == 0 or i == total:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = i / elapsed if elapsed > 0 else 0\n",
    "                remaining = (total - i) / rate if rate > 0 else 0\n",
    "                print(f\"Progress: {i}/{total} ({i/total*100:.1f}%) - \"\n",
    "                      f\"Rate: {rate:.1f} req/s - \"\n",
    "                      f\"ETA: {remaining:.0f}s\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            if i < total:  # Don't delay after last request\n",
    "                time.sleep(rate_limit_delay)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing comment {i}: {e}\")\n",
    "            # Add error result and continue\n",
    "            results.append({\n",
    "                \"comment\": comment,\n",
    "                \"sentiment\": 0,\n",
    "                \"input_tokens\": 0,\n",
    "                \"output_tokens\": 0,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"Batch prediction completed in {elapsed:.1f}s\")\n",
    "    print(f\"\\nCompleted in {elapsed:.1f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "logger.info(\"Batch prediction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Batch Prediction\n",
    "\n",
    "Process all comments and collect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch prediction\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Batch Sentiment Prediction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prediction_results = batch_predict(\n",
    "    comments=comments,\n",
    "    client=openai_client,\n",
    "    prompt_engine=prompt_engine,\n",
    "    rate_limit_delay=config.rate_limit_delay\n",
    ")\n",
    "\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "logger.info(f\"Collected {len(prediction_results)} prediction results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Results to DataFrame\n",
    "\n",
    "Structure the prediction results for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(prediction_results)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Prediction Results Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total predictions: {len(results_df)}\")\n",
    "print(f\"Successful predictions: {results_df['error'].isna().sum()}\")\n",
    "print(f\"Failed predictions: {results_df['error'].notna().sum()}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Display first few results\n",
    "print(\"Sample Results:\")\n",
    "display(results_df[['comment', 'sentiment', 'input_tokens', 'output_tokens']].head(10))\n",
    "\n",
    "logger.info(\"Results converted to DataFrame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Cost Tracking <a id='cost-tracking'></a>\n",
    "\n",
    "Track token usage and estimate API costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CostTracker Class\n",
    "\n",
    "Create a class to track token usage and calculate costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostTracker:\n",
    "    \"\"\"Tracks token usage and estimates API costs.\n",
    "    \n",
    "    This class handles:\n",
    "    - Recording token usage per request\n",
    "    - Aggregating total tokens\n",
    "    - Calculating costs based on model pricing\n",
    "    - Displaying usage summaries\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"Initialize cost tracker with model-specific pricing.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the OpenAI model\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Pricing per 1K tokens (as of 2024)\n",
    "        self.pricing = {\n",
    "            \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
    "            \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03},\n",
    "            \"gpt-3.5-turbo\": {\"input\": 0.0015, \"output\": 0.002},\n",
    "            \"gpt-3.5-turbo-16k\": {\"input\": 0.003, \"output\": 0.004}\n",
    "        }\n",
    "        \n",
    "        # Get pricing for model (default to gpt-3.5-turbo if not found)\n",
    "        self.model_pricing = self.pricing.get(model_name, self.pricing[\"gpt-3.5-turbo\"])\n",
    "        \n",
    "        # Initialize counters\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "        self.request_count = 0\n",
    "        \n",
    "        self.logger.info(f\"CostTracker initialized for model: {model_name}\")\n",
    "    \n",
    "    def add_request(self, input_tokens: int, output_tokens: int) -> None:\n",
    "        \"\"\"Record token usage for a request.\n",
    "        \n",
    "        Args:\n",
    "            input_tokens: Number of input tokens\n",
    "            output_tokens: Number of output tokens\n",
    "        \"\"\"\n",
    "        self.total_input_tokens += input_tokens\n",
    "        self.total_output_tokens += output_tokens\n",
    "        self.request_count += 1\n",
    "    \n",
    "    def add_results(self, results: List[Dict[str, Any]]) -> None:\n",
    "        \"\"\"Add token usage from a list of results.\n",
    "        \n",
    "        Args:\n",
    "            results: List of prediction result dictionaries\n",
    "        \"\"\"\n",
    "        for result in results:\n",
    "            if result.get('error') is None:\n",
    "                self.add_request(\n",
    "                    result.get('input_tokens', 0),\n",
    "                    result.get('output_tokens', 0)\n",
    "                )\n",
    "    \n",
    "    def get_total_tokens(self) -> Dict[str, int]:\n",
    "        \"\"\"Get total input and output tokens.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with input, output, and total tokens\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"input_tokens\": self.total_input_tokens,\n",
    "            \"output_tokens\": self.total_output_tokens,\n",
    "            \"total_tokens\": self.total_input_tokens + self.total_output_tokens\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self) -> float:\n",
    "        \"\"\"Calculate estimated cost in USD.\n",
    "        \n",
    "        Returns:\n",
    "            Estimated cost in dollars\n",
    "        \"\"\"\n",
    "        input_cost = (self.total_input_tokens / 1000) * self.model_pricing[\"input\"]\n",
    "        output_cost = (self.total_output_tokens / 1000) * self.model_pricing[\"output\"]\n",
    "        return input_cost + output_cost\n",
    "    \n",
    "    def display_summary(self) -> None:\n",
    "        \"\"\"Display usage and cost summary.\"\"\"\n",
    "        tokens = self.get_total_tokens()\n",
    "        cost = self.estimate_cost()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Cost and Usage Summary\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nModel: {self.model_name}\")\n",
    "        print(f\"Total Requests: {self.request_count}\")\n",
    "        \n",
    "        print(\"\\nToken Usage:\")\n",
    "        print(f\"  Input Tokens: {tokens['input_tokens']:,}\")\n",
    "        print(f\"  Output Tokens: {tokens['output_tokens']:,}\")\n",
    "        print(f\"  Total Tokens: {tokens['total_tokens']:,}\")\n",
    "        \n",
    "        print(\"\\nPricing (per 1K tokens):\")\n",
    "        print(f\"  Input: ${self.model_pricing['input']:.4f}\")\n",
    "        print(f\"  Output: ${self.model_pricing['output']:.4f}\")\n",
    "        \n",
    "        print(\"\\nEstimated Cost:\")\n",
    "        print(f\"  Input Cost: ${(tokens['input_tokens'] / 1000) * self.model_pricing['input']:.4f}\")\n",
    "        print(f\"  Output Cost: ${(tokens['output_tokens'] / 1000) * self.model_pricing['output']:.4f}\")\n",
    "        print(f\"  Total Cost: ${cost:.4f}\")\n",
    "        \n",
    "        if self.request_count > 0:\n",
    "            avg_cost = cost / self.request_count\n",
    "            print(f\"\\nAverage Cost per Request: ${avg_cost:.6f}\")\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "logger.info(\"CostTracker class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Costs for Predictions\n",
    "\n",
    "Calculate and display the cost of the batch prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cost tracker\n",
    "cost_tracker = CostTracker(config.model_name)\n",
    "\n",
    "# Add results to tracker\n",
    "cost_tracker.add_results(prediction_results)\n",
    "\n",
    "# Display summary\n",
    "cost_tracker.display_summary()\n",
    "\n",
    "logger.info(\"Cost tracking completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Results Analysis <a id='results-analysis'></a>\n",
    "\n",
    "Analyze and visualize sentiment prediction results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResultsAnalyzer Class\n",
    "\n",
    "Create a class to analyze and visualize prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsAnalyzer:\n",
    "    \"\"\"Analyzes and visualizes sentiment prediction results.\n",
    "    \n",
    "    This class handles:\n",
    "    - Computing sentiment distribution\n",
    "    - Calculating accuracy metrics\n",
    "    - Creating visualizations\n",
    "    - Displaying sample predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the ResultsAnalyzer.\"\"\"\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.info(\"ResultsAnalyzer initialized\")\n",
    "    \n",
    "    def compute_distribution(self, results_df: pd.DataFrame) -> Dict[int, int]:\n",
    "        \"\"\"Compute sentiment distribution.\n",
    "        \n",
    "        Args:\n",
    "            results_df: DataFrame with prediction results\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping sentiment to count\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Computing sentiment distribution\")\n",
    "        \n",
    "        distribution = results_df['sentiment'].value_counts().to_dict()\n",
    "        \n",
    "        # Ensure all sentiments are present\n",
    "        for sentiment in [-1, 0, 1]:\n",
    "            if sentiment not in distribution:\n",
    "                distribution[sentiment] = 0\n",
    "        \n",
    "        self.logger.info(f\"Distribution: {distribution}\")\n",
    "        return distribution\n",
    "    \n",
    "    def calculate_metrics(self, results_df: pd.DataFrame, \n",
    "                         ground_truth: Optional[pd.Series] = None) -> Dict[str, float]:\n",
    "        \"\"\"Calculate accuracy metrics if ground truth available.\n",
    "        \n",
    "        Args:\n",
    "            results_df: DataFrame with prediction results\n",
    "            ground_truth: Optional series with true sentiment labels\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with accuracy metrics\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Calculating metrics\")\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        if ground_truth is not None:\n",
    "            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "            \n",
    "            predictions = results_df['sentiment']\n",
    "            \n",
    "            metrics['accuracy'] = accuracy_score(ground_truth, predictions)\n",
    "            metrics['precision'] = precision_score(ground_truth, predictions, average='weighted', zero_division=0)\n",
    "            metrics['recall'] = recall_score(ground_truth, predictions, average='weighted', zero_division=0)\n",
    "            metrics['f1'] = f1_score(ground_truth, predictions, average='weighted', zero_division=0)\n",
    "            \n",
    "            self.logger.info(f\"Metrics: {metrics}\")\n",
    "        else:\n",
    "            self.logger.info(\"No ground truth provided, skipping metrics calculation\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_distribution(self, distribution: Dict[int, int]) -> None:\n",
    "        \"\"\"Create bar chart of sentiment distribution.\n",
    "        \n",
    "        Args:\n",
    "            distribution: Dictionary mapping sentiment to count\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Creating distribution plot\")\n",
    "        \n",
    "        # Prepare data\n",
    "        sentiments = [-1, 0, 1]\n",
    "        labels = ['Negative', 'Neutral', 'Positive']\n",
    "        counts = [distribution.get(s, 0) for s in sentiments]\n",
    "        colors = ['#ef5350', '#ffa726', '#66bb6a']\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Create bars\n",
    "        bars = ax.bar(labels, counts, color=colors, alpha=0.8, edgecolor='black')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{count}\\n({count/sum(counts)*100:.1f}%)',\n",
    "                   ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Styling\n",
    "        ax.set_xlabel('Sentiment', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Sentiment Distribution', fontsize=14, fontweight='bold', pad=20)\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        self.logger.info(\"Distribution plot created\")\n",
    "    \n",
    "    def display_samples(self, results_df: pd.DataFrame, n: int = 10) -> None:\n",
    "        \"\"\"Display sample predictions.\n",
    "        \n",
    "        Args:\n",
    "            results_df: DataFrame with prediction results\n",
    "            n: Number of samples to display\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Displaying {n} sample predictions\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Sample Predictions\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        sentiment_map = {-1: 'Negative', 0: 'Neutral', 1: 'Positive'}\n",
    "        \n",
    "        for i, row in results_df.head(n).iterrows():\n",
    "            sentiment_label = sentiment_map.get(row['sentiment'], 'Unknown')\n",
    "            comment = row['comment'][:150] + \"...\" if len(row['comment']) > 150 else row['comment']\n",
    "            \n",
    "            print(f\"\\n{i+1}. Sentiment: {sentiment_label} ({row['sentiment']})\")\n",
    "            print(f\"   Comment: {comment}\")\n",
    "            if row.get('error'):\n",
    "                print(f\"   Error: {row['error']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "        \n",
    "        self.logger.info(\"Sample predictions displayed\")\n",
    "    \n",
    "    def export_results(self, results_df: pd.DataFrame, output_path: str) -> None:\n",
    "        \"\"\"Export results to CSV.\n",
    "        \n",
    "        Args:\n",
    "            results_df: DataFrame with prediction results\n",
    "            output_path: Path to save CSV file\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Exporting results to: {output_path}\")\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            self.logger.info(f\"Created output directory: {output_dir}\")\n",
    "        \n",
    "        # Export to CSV\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        self.logger.info(f\"Results exported successfully to {output_path}\")\n",
    "        print(f\"\\nResults saved to: {output_path}\")\n",
    "\n",
    "logger.info(\"ResultsAnalyzer class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Results\n",
    "\n",
    "Compute sentiment distribution and display statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = ResultsAnalyzer()\n",
    "\n",
    "# Compute distribution\n",
    "distribution = analyzer.compute_distribution(results_df)\n",
    "\n",
    "# Display distribution\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sentiment Distribution\")\n",
    "print(\"=\"*60)\n",
    "total = sum(distribution.values())\n",
    "print(f\"\\nNegative (-1): {distribution[-1]} ({distribution[-1]/total*100:.1f}%)\")\n",
    "print(f\"Neutral (0):   {distribution[0]} ({distribution[0]/total*100:.1f}%)\")\n",
    "print(f\"Positive (1):  {distribution[1]} ({distribution[1]/total*100:.1f}%)\")\n",
    "print(f\"\\nTotal: {total}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "logger.info(\"Distribution analysis completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Distribution\n",
    "\n",
    "Create a bar chart showing sentiment distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n",
    "analyzer.plot_distribution(distribution)\n",
    "\n",
    "logger.info(\"Distribution visualization created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Sample Predictions\n",
    "\n",
    "Show sample predictions with original comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display samples\n",
    "analyzer.display_samples(results_df, n=10)\n",
    "\n",
    "logger.info(\"Sample predictions displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export Results <a id='export-results'></a>\n",
    "\n",
    "Save prediction results to CSV for downstream analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Output Filename\n",
    "\n",
    "Create a descriptive filename with timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "dataset_name = os.path.basename(config.data_file).replace('.csv', '')\n",
    "output_filename = f\"{dataset_name}_sentiment_predictions_{timestamp}.csv\"\n",
    "output_path = os.path.join(config.output_dir, output_filename)\n",
    "\n",
    "print(f\"\\nOutput file: {output_path}\")\n",
    "\n",
    "logger.info(f\"Generated output filename: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Results to CSV\n",
    "\n",
    "Save the prediction results with all metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "analyzer.export_results(results_df, output_path)\n",
    "\n",
    "# Display file info\n",
    "if os.path.exists(output_path):\n",
    "    file_size = os.path.getsize(output_path) / 1024  # KB\n",
    "    print(f\"File size: {file_size:.2f} KB\")\n",
    "    print(f\"Rows exported: {len(results_df)}\")\n",
    "    print(f\"Columns: {list(results_df.columns)}\")\n",
    "\n",
    "logger.info(\"Results exported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Export (Optional)\n",
    "\n",
    "Load the exported file to verify data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load exported file to verify\n",
    "if os.path.exists(output_path):\n",
    "    loaded_df = pd.read_csv(output_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Export Verification\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Original rows: {len(results_df)}\")\n",
    "    print(f\"Loaded rows: {len(loaded_df)}\")\n",
    "    print(f\"Match: {len(results_df) == len(loaded_df)}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"First few rows of exported file:\")\n",
    "    display(loaded_df.head())\n",
    "    \n",
    "    logger.info(\"Export verification completed\")\n",
    "else:\n",
    "    logger.warning(f\"Export file not found: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary <a id='summary'></a>\n",
    "\n",
    "Summary of the sentiment analysis workflow and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Selection Helper\n",
    "\n",
    "List available datasets for easy selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_available_datasets(directory: str = \"reviews\") -> List[str]:\n",
    "    \"\"\"List available CSV files in the reviews directory.\n",
    "    \n",
    "    Args:\n",
    "        directory: Directory to search for CSV files\n",
    "    \n",
    "    Returns:\n",
    "        List of CSV filenames\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        logger.warning(f\"Directory not found: {directory}\")\n",
    "        return []\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "    csv_files.sort()\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "# List available datasets\n",
    "available_datasets = list_available_datasets()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Available Datasets\")\n",
    "print(\"=\"*60)\n",
    "for i, dataset in enumerate(available_datasets, 1):\n",
    "    print(f\"{i}. {dataset}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "logger.info(f\"Found {len(available_datasets)} available datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "\n",
    "Display key findings and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENTIMENT ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n📊 Dataset Information:\")\n",
    "print(f\"  File: {config.data_file}\")\n",
    "print(f\"  Total Reviews: {len(reviews_df)}\")\n",
    "print(f\"  Comments Analyzed: {len(comments)}\")\n",
    "\n",
    "print(\"\\n🤖 Model Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Temperature: {config.temperature}\")\n",
    "print(f\"  Max Tokens: {config.max_tokens}\")\n",
    "\n",
    "print(\"\\n📈 Sentiment Distribution:\")\n",
    "total = sum(distribution.values())\n",
    "print(f\"  Positive: {distribution[1]} ({distribution[1]/total*100:.1f}%)\")\n",
    "print(f\"  Neutral: {distribution[0]} ({distribution[0]/total*100:.1f}%)\")\n",
    "print(f\"  Negative: {distribution[-1]} ({distribution[-1]/total*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n💰 Cost Summary:\")\n",
    "tokens = cost_tracker.get_total_tokens()\n",
    "cost = cost_tracker.estimate_cost()\n",
    "print(f\"  Total Tokens: {tokens['total_tokens']:,}\")\n",
    "print(f\"  Estimated Cost: ${cost:.4f}\")\n",
    "print(f\"  Cost per Review: ${cost/len(comments):.6f}\")\n",
    "\n",
    "print(\"\\n💾 Output:\")\n",
    "print(f\"  Results saved to: {output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Analysis Complete!\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "logger.info(\"Sentiment analysis workflow completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations and Next Steps\n",
    "\n",
    "**Cost Optimization:**\n",
    "- Use `gpt-3.5-turbo` for large-scale analysis (10-20x cheaper than GPT-4)\n",
    "- Sample large datasets instead of processing all reviews\n",
    "- Adjust `rate_limit_delay` based on your API tier\n",
    "\n",
    "**Accuracy Improvement:**\n",
    "- Fine-tune few-shot examples for your specific domain\n",
    "- Experiment with different temperature settings\n",
    "- Consider using GPT-4 for higher accuracy (at higher cost)\n",
    "\n",
    "**Production Deployment:**\n",
    "- Implement caching for duplicate comments\n",
    "- Add database integration for persistent storage\n",
    "- Set up monitoring and alerting for API errors\n",
    "- Implement batch processing with queues for large datasets\n",
    "\n",
    "**Further Analysis:**\n",
    "- Perform aspect-based sentiment analysis (location, cleanliness, host)\n",
    "- Compare sentiment across different cities\n",
    "- Analyze temporal trends in sentiment\n",
    "- Extract key phrases from positive and negative reviews\n",
    "\n",
    "**Model Comparison:**\n",
    "- Compare results from different OpenAI models\n",
    "- Benchmark against other sentiment analysis services (IBM Watson, AWS Comprehend)\n",
    "- Evaluate cost vs. accuracy tradeoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References and Resources\n",
    "\n",
    "**OpenAI Documentation:**\n",
    "- API Reference: https://platform.openai.com/docs/api-reference\n",
    "- Pricing: https://openai.com/pricing\n",
    "- Best Practices: https://platform.openai.com/docs/guides/production-best-practices\n",
    "\n",
    "**Prompt Engineering:**\n",
    "- Prompt Engineering Guide: https://www.promptingguide.ai/\n",
    "- Few-Shot Learning: https://platform.openai.com/docs/guides/prompt-engineering\n",
    "\n",
    "**Dataset:**\n",
    "- Inside Airbnb: http://insideairbnb.com/get-the-data.html\n",
    "\n",
    "**Related Libraries:**\n",
    "- OpenAI Python SDK: https://github.com/openai/openai-python\n",
    "- Pandas: https://pandas.pydata.org/docs/\n",
    "- Matplotlib: https://matplotlib.org/stable/contents.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
